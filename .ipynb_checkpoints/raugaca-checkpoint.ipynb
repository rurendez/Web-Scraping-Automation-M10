{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470bf8c1",
   "metadata": {},
   "source": [
    "raugaca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42949bd6",
   "metadata": {},
   "source": [
    "**BeautifulSoup\n",
    "http://quotes.toscrape.com**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL_1 = \"http://quotes.toscrape.com\"\n",
    "page_1 = requests.get(URL_1)\n",
    "print(page_1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34054cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_1 = BeautifulSoup(page_1.content, \"html.parser\")\n",
    "results_1 = soup_1.find(class_=\"container\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af656849",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_1 = results_1.find_all(\"div\", class_=\"quote\")\n",
    "\n",
    "for element_1 in elements_1:\n",
    "    cita = element_1.find(class_=\"text\")\n",
    "    autor = element_1.find(class_=\"author\")\n",
    "    etiquetes=element_1.find(class_=\"tags\")\n",
    "    eti= etiquetes.text.strip().replace('\\n', ', ')\n",
    "    eti= eti.replace(',             , ', ' ')\n",
    "    \n",
    "    print(cita.text.strip())\n",
    "    print(autor.text.strip())\n",
    "    print(eti)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373434ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En aquesta web per variar una mica descarregarem totes les adreces web que surten.\n",
    "\n",
    "URL_2 = \"https://www.bolsamadrid.es/\"\n",
    "page_2 = requests.get(URL_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_2 = BeautifulSoup(page_2.content, \"html.parser\")\n",
    "results_2 = soup_2.find(class_=\"Menu2 MenuSubOp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bc200",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_2.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf90f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for link in soup_2.find_all('a'): \n",
    "    k=link.get('href')\n",
    "    if (isinstance(k, str)==True):\n",
    "        if (k[:4]=='http'):\n",
    "             print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treurem una taula amb els 10 països més poblats del món segons les dades de 2022.\n",
    "URL_3 = \"https://es.wikipedia.org/wiki/Poblaci%C3%B3n_mundial\"\n",
    "page_3 = requests.get(URL_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_3 = BeautifulSoup(page_3.content, \"html.parser\")\n",
    "tables = soup_3.find_all('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6357df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busquem l'índex de la taula que volem extreure\n",
    "for index,table in enumerate(tables):\n",
    "    if (\"Población 2022\" in str(table)):\n",
    "         table_index = index\n",
    "print(table_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e83676",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables[table_index].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "poblacion_data = pd.DataFrame(columns=[\"Puesto\", \"País\", \"Población 2022\"])\n",
    "\n",
    "z=2\n",
    "for row in tables[table_index].tbody.find_all(\"tr\"):\n",
    "    col = row.find_all(\"td\")\n",
    "    if (col != []) and z<10:\n",
    "        puesto = col[0].text.strip()\n",
    "        pais = col[1].text.strip()\n",
    "        poblacion = col[2].text.strip()\n",
    "        poblacion_data = poblacion_data.append({\"Puesto\":puesto, \"País\":pais, \"Población 2022\":poblacion}, ignore_index=True)\n",
    "        z+= 1\n",
    "\n",
    "poblacion_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d67737e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T04:17:33.079552Z",
     "start_time": "2023-02-16T04:17:33.071553Z"
    }
   },
   "source": [
    "**Selenium\n",
    "http://quotes.toscrape.com**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535fc38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb162465",
   "metadata": {},
   "outputs": [],
   "source": [
    "options_chr = Options()\n",
    "browser_chr = webdriver.Chrome(options=options_chr)\n",
    "browser_chr.get('http://quotes.toscrape.com')\n",
    "\n",
    "results = browser_chr.find_elements(By.CLASS_NAME,'quote')\n",
    "for value in results:\n",
    "    print('\\n', value.text)\n",
    "\n",
    "browser_chr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012ce82",
   "metadata": {},
   "source": [
    "**https://www.bolsamadrid.es**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# En aquesta web per variar una mica descarregarem totes les adreces web que surten.\n",
    "\n",
    "options_chr = Options()\n",
    "browser_chr = webdriver.Chrome(options=options_chr)\n",
    "browser_chr.get('https://www.bolsamadrid.es/')\n",
    "\n",
    "results = browser_chr.find_elements(By.XPATH,'//a[@href]')\n",
    "\n",
    "i=0\n",
    "for result in results:\n",
    "    try:\n",
    "        k=result.get_attribute(\"href\")\n",
    "        if (isinstance(k, str)==True):\n",
    "            if (k[:4]=='http'):\n",
    "                print(k)\n",
    "                i+=1   \n",
    "    except:\n",
    "        pass\n",
    "print(\"S'han descarregat \", i ,\" adreces de la web.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54f0c0",
   "metadata": {},
   "source": [
    "**www.wikipedia.es**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a407112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treurem una taula amb els 10 països més poblats del món segons les dades de 2022.\n",
    "options_chr = Options()\n",
    "browser_chr_3 = webdriver.Chrome(options=options_chr)\n",
    "browser_chr_3.get('https://es.wikipedia.org/wiki/Poblaci%C3%B3n_mundial')\n",
    "\n",
    "tables = browser_chr_3.find_elements(By.TAG_NAME,'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busquem l'índex de la taula que volem extreure\n",
    "for index,table in enumerate(tables):\n",
    "    if (\"Población 2022\" in str(table)):\n",
    "         table_index = index\n",
    "print(table_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad113ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "poblacion_data = pd.DataFrame(columns=[\"Puesto\", \"País\", \"Población 2022\"])\n",
    "table=tables[table_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f08c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=2\n",
    "for row in table.find_elements(By.TAG_NAME,'tr'):\n",
    "    col = row.find_elements(By.TAG_NAME,'td')\n",
    "    if (col != []) and z<10:\n",
    "        puesto = col[0].text\n",
    "        pais = col[1].text\n",
    "        poblacion = col[2].text\n",
    "        poblacion_data = poblacion_data.append({\"Puesto\":puesto, \"País\":pais, \"Población 2022\":poblacion}, ignore_index=True)\n",
    "        z+= 1\n",
    "\n",
    "poblacion_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2529f18",
   "metadata": {},
   "source": [
    "**Exercici 2\n",
    "Documenta en un Word el teu conjunt de dades generat amb la informació que tenen els diferents arxius de Kaggle.**\n",
    "\n",
    "Per saber més-> A manera d'exemple del que es demana pots consultar aquest enllaç: -> https://www.kaggle.com/datasets/vivovinco/20212022-football-team-stats.\n",
    "\n",
    "2.1.- Quotes\n",
    "Context\n",
    "Extracted data contains quotes from famous personalities including, writers, scientifics, actors,…\n",
    "Website is used for web scraping tests purposes.\n",
    "\n",
    "Content\n",
    "Website has many quotes but extracted information includes 10 sets with:\n",
    "\n",
    "Quotes\n",
    "Author\n",
    "Tags (related to quotes and authors)\n",
    "Acknowledgements\n",
    "Data extracted from: http://quotes.toscrape.com\n",
    "\n",
    "2.2.- Bolsa Madrid\n",
    "Context\n",
    "Extracted urls included in the website frontpage of Madrid stock exchange, excluding the ones not starting with \"http\".\n",
    "\n",
    "Content\n",
    "236 Urls extracted including internal and external.\n",
    "\n",
    "Acknowledgements\n",
    "Data extracted from frontpage: https://www.bolsamadrid.es\n",
    "\n",
    "2.3.-Wikipedia\n",
    "Context\n",
    "This url contains wide information about world population, including 8 tables. Original table to extract data includes all countries population but we only show topten.\n",
    "\n",
    "Content\n",
    "TopTen World largest population countries includings following columns:\n",
    "\n",
    "Rank\n",
    "Country\n",
    "Population\n",
    "Acknowledgements\n",
    "Data extracted from wikipedia world population:\n",
    "\"https://es.wikipedia.org/wiki/Poblaci%C3%B3n_mundial\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718aed7b",
   "metadata": {},
   "source": [
    "**Exercici 3\n",
    "Tria una pàgina web que tu vulguis i realitza web scraping mitjançant la llibreria Selenium primer i Scrapy després.**\n",
    "\n",
    "Selenium: Primerament provo de fer webscraping a la web de Infojobs i trobo que té una protecció molt forta contra l'scrapping i de fet no em deixa cridar la url des de Python i la solució que trobo és obrir una sesió de proba del Chrome i conectar-me amb Selenium amb la ubicació Host i entrar manualment les urls per després poder executar el codi per fer l'scraping.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91187138",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fechalreves=str(datetime.now().year) + str(datetime.now().month)+ str(datetime.now().day)\n",
    "\n",
    "opt=Options()\n",
    "opt.add_experimental_option(\"debuggerAddress\", 'localhost:8989')\n",
    "browser_chr = webdriver.Chrome(executable_path=\"C:\\\\RAUL\\\\NISSAN\\\\GENERAL\\\\USB\\\\WebScraping\\\\chromedriver.exe\", chrome_options=opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ebc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "Empresas = browser_chr.find_elements(By.CLASS_NAME,'ij-OfferCardContent-description-subtitle-link')\n",
    "Puestos = browser_chr.find_elements(By.CLASS_NAME,'ij-OfferCardContent-description-title-link')\n",
    "Links = browser_chr.find_elements(By.CLASS_NAME,'ij-OfferCardContent-description-title-link')\n",
    "Lugares = browser_chr.find_elements(By.CLASS_NAME,'ij-OfferCardContent-description-list-item-truncate')\n",
    "\n",
    "Trabajos = pd.DataFrame(columns=[\"Empresa\", \"Puesto\", 'Fecha', \"Link\", 'Ubicación'])\n",
    "\n",
    "for i in range(len(Empresas)):\n",
    "    Trabajos = Trabajos.append({\"Empresa\":Empresas[i].text, \"Puesto\":Puestos[i].text,\n",
    "        \"Fecha\":pd.datetime.now().strftime(\"%d/%m/%Y\"),\"Link\":Links[i].get_attribute(\"href\"), \n",
    "        'Ubicación':Lugares[i].text}, ignore_index=True)\n",
    "\n",
    "Trabajos.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d25ba6",
   "metadata": {},
   "source": [
    "**Scrapy: a l'hora d'intentar aplicar l'scrapy a la web d'infojobs trobo igualment moltes dificultats i no trobo la forma per conectar-me amb la ruta host, per tant faig l'exercici amb la web de cites de l'Exercici 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51359564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99437a5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# define class to save results as json file \n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('quotes_result.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "    \n",
    "import logging\n",
    "\n",
    "# construct the spider to get the info we want \n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/',\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
    "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
    "        'FEED_URI': 'quotes_result.json'                       # Used for pipeline 2\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'Quote': quote.css('span.text::text').extract_first(),\n",
    "                'Author': quote.css('small.author::text').extract_first(),\n",
    "                'Tags': quote.css('a.tag::text').extract(),\n",
    "            }\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36'\n",
    "})\n",
    "\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e582bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# we can open the json file to a dataframe \n",
    "quotes_df = pd.read_json('quotes_result.jl', lines=True)\n",
    "quotes_df.head()\n",
    "quotes_df.style.set_properties(subset=['Tags'], **{'width': '300px'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
